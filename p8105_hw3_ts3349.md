p8105\_hw3\_ts3349
================
Tessa Senders
10/6/2020

``` r
library(tidyverse)
```

    ## -- Attaching packages ------------------------------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ---------------------------------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
library(hexbin)
library(patchwork)
```

## Problem 1

``` r
data("instacart")
```

This dataset contains 1384617 rows and 15 columns. Observations are the
level of items in orders by user. There are user/order variables-user
ID, order ID, order day, and order hour. There are also item
variables-name, aisle, department, and some numeric codes.

How many aisles and which are most items from?

``` r
aisles_df <- instacart %>%
  count(aisle) %>%
  arrange(desc(n))

head(aisles_df)
```

    ## # A tibble: 6 x 2
    ##   aisle                              n
    ##   <chr>                          <int>
    ## 1 fresh vegetables              150609
    ## 2 fresh fruits                  150473
    ## 3 packaged vegetables fruits     78493
    ## 4 yogurt                         55240
    ## 5 packaged cheese                41699
    ## 6 water seltzer sparkling water  36617

There are 134 aisles and the most items are ordered from the fresh
vegetables and fresh fruits aisles.

Plot

``` r
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))
```

![](p8105_hw3_ts3349_files/figure-gfm/prob%201%20plot-1.png)<!-- -->

Table

``` r
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Second Table

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>%
  knitr::kable()
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

| product\_name    |        0 |        1 |        2 |        3 |        4 |        5 |        6 |
| :--------------- | -------: | -------: | -------: | -------: | -------: | -------: | -------: |
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 | 15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 | 14.25000 | 11.55172 | 12.78431 | 11.93750 |

## Problem 2

``` r
activity_df <- read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day", 
    values_to = "activity_count"
  ) %>%
  mutate(day = str_to_lower(day)) %>%
  mutate(day_temp = as.numeric(factor(day, levels = str_c(c("saturday", "sunday", "monday", "tuesday", "wednesday", "thursday", "friday"))))) %>%
  mutate(day_of_week = if_else(day_temp == 1 | day_temp == 2, "weekend", "weekday")) %>%
  select(!day_temp)%>%
  mutate(minute_of_day = as.numeric(str_remove(minute_of_day, "activity_"))) %>%
  relocate(week, day_of_week, day, day_id, minute_of_day, activity_count)
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
head(activity_df)  
```

    ## # A tibble: 6 x 6
    ##    week day_of_week day    day_id minute_of_day activity_count
    ##   <dbl> <chr>       <chr>   <dbl>         <dbl>          <dbl>
    ## 1     1 weekday     friday      1             1           88.4
    ## 2     1 weekday     friday      1             2           82.2
    ## 3     1 weekday     friday      1             3           64.4
    ## 4     1 weekday     friday      1             4           70.0
    ## 5     1 weekday     friday      1             5           75.0
    ## 6     1 weekday     friday      1             6           66.3

This data set contains information from a 63 year-old male with BMI 25,
who was admitted to the Advanced Cardiac Care Center of Columbia
University Medical Center and diagnosed with congestive heart failure.
For each minute of each day for a total of 5 weeks (35 days), his
activity count was recorded. The data set contains 6 variables. These
variables include week(week number 1-5), day\_of\_week(whether the day
was a weekday or weekend day), day(Sunday-Saturday), day\_id(1-35),
minute\_of\_day, and activity\_count. There are a total of 50400 rows in
the final data set. The median number for activity\_count for example is
74.

``` r
activity_table_df <- activity_df %>% 
  group_by(day_id, week) %>%
  summarize(total_activity = sum(activity_count)) %>%
  relocate(week)
```

    ## `summarise()` regrouping output by 'day_id' (override with `.groups` argument)

``` r
activity_table_df %>%
  knitr::kable()
```

| week | day\_id | total\_activity |
| ---: | ------: | --------------: |
|    1 |       1 |       480542.62 |
|    1 |       2 |        78828.07 |
|    1 |       3 |       376254.00 |
|    1 |       4 |       631105.00 |
|    1 |       5 |       355923.64 |
|    1 |       6 |       307094.24 |
|    1 |       7 |       340115.01 |
|    2 |       8 |       568839.00 |
|    2 |       9 |       295431.00 |
|    2 |      10 |       607175.00 |
|    2 |      11 |       422018.00 |
|    2 |      12 |       474048.00 |
|    2 |      13 |       423245.00 |
|    2 |      14 |       440962.00 |
|    3 |      15 |       467420.00 |
|    3 |      16 |       685910.00 |
|    3 |      17 |       382928.00 |
|    3 |      18 |       467052.00 |
|    3 |      19 |       371230.00 |
|    3 |      20 |       381507.00 |
|    3 |      21 |       468869.00 |
|    4 |      22 |       154049.00 |
|    4 |      23 |       409450.00 |
|    4 |      24 |         1440.00 |
|    4 |      25 |       260617.00 |
|    4 |      26 |       340291.00 |
|    4 |      27 |       319568.00 |
|    4 |      28 |       434460.00 |
|    5 |      29 |       620860.00 |
|    5 |      30 |       389080.00 |
|    5 |      31 |         1440.00 |
|    5 |      32 |       138421.00 |
|    5 |      33 |       549658.00 |
|    5 |      34 |       367824.00 |
|    5 |      35 |       445366.00 |

Looking at the data there are a couple clear outliers where the total
activity count is below 2000. Most of the total activity count values
for each day are between 130,000 and 700,000. For the first week most
total activity count values were around 300,000. The second week, most
of the total activity count values seem to increase to around 400,000.
Overtime, however, the values become more sporadic dipping to around
150,000 and then increasing to as high as around 600,000.

``` r
activity_df  %>% 
  mutate(day = as.factor(day)) %>%
  mutate(day = factor(day, levels = str_c(c("sunday", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday")))) %>%
  ggplot(aes(x = minute_of_day, y = activity_count, color = day)) + 
  geom_line(alpha = 0.5) + 
  labs(
    title = "Activity Over the Course of the Day",
    x = "Minute of the Day",
    y = "Activity Count",
    caption = "Data from the accelerometer data collected on a 63 year-old male with BMI 25"
  ) +
  guides(color = guide_legend("Day"))
```

![](p8105_hw3_ts3349_files/figure-gfm/problem%202%20bullet%203-1.png)<!-- -->
The first few hundred minutes of the day for all days of the week seem
to have the lowest activity counts probably because the man is sleeping.
For most days of the week the activity count seems to spike towards the
end of the day around the 1250th minute of the day. Tuesdays through
Thursdays seem to have some of the overall highest activity counts
during the 450th minute and the 1200th minute of the day. Mondaysâ€™
activity count however typically peaks around the 750th minute of the
day.

## Problem 3

``` r
data("ny_noaa")
```

``` r
ny_noaa_df <- ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into=c("year", "month", "day"), sep = "-") %>%
  mutate(year = as.numeric(year)) %>%
  mutate(month = as.numeric(month)) %>%
  mutate(day = as.numeric(day)) %>%
  mutate(prcp_cm = prcp / 100) %>%
  mutate(snow_cm = snow / 10) %>%
  mutate(snwd_cm = snwd / 10) %>%
  mutate(tmax = as.numeric(tmax) / 10) %>%
  mutate(tmin = as.numeric(tmin) / 10) %>%
  select(-prcp, -snow, -snwd) 

head(ny_noaa_df)
```

    ## # A tibble: 6 x 9
    ##   id           year month   day  tmax  tmin prcp_cm snow_cm snwd_cm
    ##   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>   <dbl>   <dbl>
    ## 1 US1NYAB0001  2007    11     1    NA    NA      NA      NA      NA
    ## 2 US1NYAB0001  2007    11     2    NA    NA      NA      NA      NA
    ## 3 US1NYAB0001  2007    11     3    NA    NA      NA      NA      NA
    ## 4 US1NYAB0001  2007    11     4    NA    NA      NA      NA      NA
    ## 5 US1NYAB0001  2007    11     5    NA    NA      NA      NA      NA
    ## 6 US1NYAB0001  2007    11     6    NA    NA      NA      NA      NA

``` r
ny_noaa_snowfall <- ny_noaa_df %>%
  group_by(snow_cm) %>%
  count() %>%
  arrange(desc(n))

head(ny_noaa_snowfall)
```

    ## # A tibble: 6 x 2
    ## # Groups:   snow_cm [6]
    ##   snow_cm       n
    ##     <dbl>   <int>
    ## 1     0   2008508
    ## 2    NA    381221
    ## 3     2.5   31022
    ## 4     1.3   23095
    ## 5     5.1   18274
    ## 6     7.6   10173

0.0 is the most commonly observed snowfall value because it does not
snow year round in New York. Most of the time it is not snowing. After
NA (missing), the next most common snowfall amounts are 2.5cm and 1.3cm.

``` r
noaa_jan_july_df <- ny_noaa_df  %>% 
  filter((month == 1) | (month == 7)) %>%
  group_by(id, year, month) %>%
  summarize(avg_max_temp = mean(tmax, na.rm=TRUE))
```

    ## `summarise()` regrouping output by 'id', 'year' (override with `.groups` argument)

``` r
noaa_jan_july_df %>%
  ggplot(aes(x = year, y = avg_max_temp, color = id)) + 
  geom_line() + 
  facet_grid(. ~ month) +
  labs(
    title = "Average Max Temperature Across Years for January vs July",
    x = "Year",
    y = "Average Max Temperature (C)",
    caption = "Data from the New York NOAA weather stations across years for January and July ") +
    theme(legend.position = "none") +
    theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))
```

    ## Warning: Removed 5640 row(s) containing missing values (geom_path).

![](p8105_hw3_ts3349_files/figure-gfm/problem%203%20bullet%202-1.png)<!-- -->
Both graphs show a zigzag pattern where the average max temperature dips
a little one year but then increases a little the next year. There seems
to be a very slight upward trend for January but overall the average max
temperatures for January seem fairly consistent. The average max
temperature in July seem very consistent overall. Some stations in both
graphs seem to always be slightly higher or slightly lower overall than
the others. In the graph for July there seems to be a station that is an
outlier in the late 80s with a large dip in the average max temperature.
In the year 2000 for January there seems to be a station with a much
higher average max temperature than the others. The max temperatures in
July are overall higher than the max temperatures in January.

``` r
tmax_vs_tmin_plot <- ny_noaa_df %>%
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex() + 
  labs(
    title = "Max Temperature vs Min Temperature for NY Stations",
    x = "Max Temperature (C)",
    y = "Min Temperature (C)",
    caption = "Data from the New York NOAA weather stations") 


snowfall_plot <- ny_noaa_df  %>% 
  mutate(snow = snow_cm *10) %>%
  filter((snow > 0) & (snow < 100)) %>%
  ggplot(aes(x = as.character(year), y = (snow))) + 
  geom_boxplot() + 
  labs(
    title = "Snowfall Across Years (Greater than 0 and Less than 100 mm)",
    x = "Year",
    y = "Snowfall (mm)",
    caption = "Data from the New York NOAA weather stations")  +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))

tmax_vs_tmin_plot / snowfall_plot
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

![](p8105_hw3_ts3349_files/figure-gfm/problem%203%20bullet%203-1.png)<!-- -->
In the first graph the average max temperature and the average min
temperature for many stations tend to be the same/similar. There are
many stations that recorded an average max temperature of around 30 and
an average minimum temperature of around 30. In the second graph, the
distribution of snowfall for each year tends to be similar except for a
few years. In 1998, 2006, and 2020 the 3rd quartiles are lower than the
rest of the years.
