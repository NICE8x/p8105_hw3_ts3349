p8105\_hw3\_ts3349
================
Tessa Senders
10/6/2020

``` r
library(tidyverse)
```

    ## -- Attaching packages ------------------------------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ---------------------------------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)
library(hexbin)
library(patchwork)
```

## Problem 1

``` r
data("instacart")
```

This dataset contains 1384617 rows and 15 columns. Observations are the
level of items in orders by user. There are user/order variables-user
ID, order ID, order day, and order hour. There are also item
variables-name, aisle, department, and some numeric codes.

How many aisles and which are most items from?

``` r
aisles_df <- instacart %>%
  count(aisle) %>%
  arrange(desc(n))

head(aisles_df)
```

    ## # A tibble: 6 x 2
    ##   aisle                              n
    ##   <chr>                          <int>
    ## 1 fresh vegetables              150609
    ## 2 fresh fruits                  150473
    ## 3 packaged vegetables fruits     78493
    ## 4 yogurt                         55240
    ## 5 packaged cheese                41699
    ## 6 water seltzer sparkling water  36617

There are 134 aisles and the most items are ordered from the fresh
vegetables and fresh fruits aisles.

Plot

``` r
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))
```

![](p8105_hw3_ts3349_files/figure-gfm/prob%201%20plot-1.png)<!-- -->

Table

``` r
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Second Table

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>%
  knitr::kable()
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

| product\_name    |        0 |        1 |        2 |        3 |        4 |        5 |        6 |
| :--------------- | -------: | -------: | -------: | -------: | -------: | -------: | -------: |
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 | 15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 | 14.25000 | 11.55172 | 12.78431 | 11.93750 |

## Problem 2

Load, tidy, and otherwise wrangle the data. Your final dataset should
include all originally observed variables and values; have useful
variable names; include a weekday vs weekend variable; and encode data
with reasonable variable classes. Describe the resulting dataset
(e.g. what variables exist, how many observations, etc).

Bullet 1: need to pivot longer with activity variables (activity count
and minute of the day) mutate to add weekend vs weekday make numeric
things numeric and make order make sense\!

``` r
activity_df <- read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute_of_day", 
    values_to = "activity_count"
  ) %>%
  mutate(day = str_to_lower(day)) %>%
  mutate(day_temp = as.numeric(factor(day, levels = str_c(c("saturday", "sunday", "monday", "tuesday", "wednesday", "thursday", "friday"))))) %>%
  mutate(day_of_week = if_else(day_temp == 1 | day_temp == 2, "weekend", "weekday")) %>%
  select(!day_temp)%>%
  mutate(minute_of_day = as.numeric(str_remove(minute_of_day, "activity_"))) %>%
  relocate(week, day_of_week, day, day_id, minute_of_day, activity_count)
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
head(activity_df)  
```

    ## # A tibble: 6 x 6
    ##    week day_of_week day    day_id minute_of_day activity_count
    ##   <dbl> <chr>       <chr>   <dbl>         <dbl>          <dbl>
    ## 1     1 weekday     friday      1             1           88.4
    ## 2     1 weekday     friday      1             2           82.2
    ## 3     1 weekday     friday      1             3           64.4
    ## 4     1 weekday     friday      1             4           70.0
    ## 5     1 weekday     friday      1             5           75.0
    ## 6     1 weekday     friday      1             6           66.3

This data set contains information from a 63 year-old male with BMI 25,
who was admitted to the Advanced Cardiac Care Center of Columbia
University Medical Center and diagnosed with congestive heart failure.
For each minute of each day for a total of 5 weeks (35 days), his
activity count was recorded. The data set contains 6 variables. These
variables include week(week number 1-5), day\_of\_week(whether the day
was a weekday or weekend day), day(Sunday-Saturday), day\_id(1-35),
minute\_of\_day, and activity\_count. There are a total of 50400 rows in
the final data set. The median number for activity\_count for example is
74.

Traditional analyses of accelerometer data focus on the total activity
over the day. Using your tidied dataset, aggregate accross minutes to
create a total activity variable for each day, and create a table
showing these totals. Are any trends apparent?

Bullet 2: aggregate something to create total (group by and summarize-
week, day or id or something, aggregate using mean or sum or something).
end up with 35 days in table, need to arrange table columns (week number
day of the week). understand whats going on in the table

``` r
activity_table_df <- activity_df %>% 
  group_by(day_id, week) %>%
  summarize(total_activity = sum(activity_count)) %>%
  relocate(week)
```

    ## `summarise()` regrouping output by 'day_id' (override with `.groups` argument)

``` r
activity_table_df %>%
  knitr::kable()
```

| week | day\_id | total\_activity |
| ---: | ------: | --------------: |
|    1 |       1 |       480542.62 |
|    1 |       2 |        78828.07 |
|    1 |       3 |       376254.00 |
|    1 |       4 |       631105.00 |
|    1 |       5 |       355923.64 |
|    1 |       6 |       307094.24 |
|    1 |       7 |       340115.01 |
|    2 |       8 |       568839.00 |
|    2 |       9 |       295431.00 |
|    2 |      10 |       607175.00 |
|    2 |      11 |       422018.00 |
|    2 |      12 |       474048.00 |
|    2 |      13 |       423245.00 |
|    2 |      14 |       440962.00 |
|    3 |      15 |       467420.00 |
|    3 |      16 |       685910.00 |
|    3 |      17 |       382928.00 |
|    3 |      18 |       467052.00 |
|    3 |      19 |       371230.00 |
|    3 |      20 |       381507.00 |
|    3 |      21 |       468869.00 |
|    4 |      22 |       154049.00 |
|    4 |      23 |       409450.00 |
|    4 |      24 |         1440.00 |
|    4 |      25 |       260617.00 |
|    4 |      26 |       340291.00 |
|    4 |      27 |       319568.00 |
|    4 |      28 |       434460.00 |
|    5 |      29 |       620860.00 |
|    5 |      30 |       389080.00 |
|    5 |      31 |         1440.00 |
|    5 |      32 |       138421.00 |
|    5 |      33 |       549658.00 |
|    5 |      34 |       367824.00 |
|    5 |      35 |       445366.00 |

Looking at the data there are a couple clear outliers where the total
activity count is below 2000. Most of the total activity count values
for each day are between 130,000 and 700,000. For the first week most
total activity count values were around 300,000. The second week, most
of the total activity count values seem to increase to around 400,000.
Overtime, however, the values become more sporadic dipping to around
150,000 and then increasing to as high as around 600,000.

Bullet 3: ggplot, need to fix data (minute on x axis, activity count on
y axis), want activity at eveyr minute of every day, geom\_line but dont
use scatterplot, use color for day of the week (aes mapping), describe
patterns in plot (which days less or more active, when are they
asleep?). have 35 squiggly lines

Accelerometer data allows the inspection activity over the course of the
day. Make a single-panel plot that shows the 24-hour activity time
courses for each day and use color to indicate day of the week. Describe
in words any patterns or conclusions you can make based on this graph.

``` r
activity_df  %>% 
  mutate(day = as.factor(day)) %>%
  mutate(day = factor(day, levels = str_c(c("sunday", "monday", "tuesday", "wednesday", "thursday", "friday", "saturday")))) %>%
  ggplot(aes(x = minute_of_day, y = activity_count, color = day)) + 
  geom_line(alpha = 0.5) + 
  labs(
    title = "Activity Over the Course of the Day",
    x = "Minute of the Day",
    y = "Activity Count",
    caption = "Data from the accelerometer data collected on a 63 year-old male with BMI 25"
  ) +
  guides(color = guide_legend("Day"))
```

![](p8105_hw3_ts3349_files/figure-gfm/problem%202%20bullet%203-1.png)<!-- -->
Describe in words any patterns or conclusions you can make based on this
graph. The first few hundred minutes of the day for all days of the week
seem to have the lowest activity counts probably because the man is
sleeping. For most days of the week the activity count seems to spike
towards the end of the day around the 1250th minute of the day. Tuesdays
through Thursdays seem to have some of the overall highest activity
counts during the 450th minute and the 1200th minute of the day.
Mondays’ activity count however typically peaks around the 750th
minute of the day.

Hints: day of the week columns will be in wrong order-use factor to fix
check if minute of the day is numeric or not? do not want character
variable

## Problem 3

``` r
data("ny_noaa")
```

Bullet 1: need to separate date to get three variables. do not need to
convert month to name. change units on variables to something useful
(want to round to integers?). need to count snowfall.

Do some data cleaning. Create separate variables for year, month, and
day. Ensure observations for temperature, precipitation, and snowfall
are given in reasonable units. For snowfall, what are the most commonly
observed values? Why?

``` r
ny_noaa_df <- ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into=c("year", "month", "day"), sep = "-") %>%
  mutate(year = as.numeric(year)) %>%
  mutate(month = as.numeric(month)) %>%
  mutate(day = as.numeric(day)) %>%
  mutate(prcp_cm = prcp / 100) %>%
  mutate(snow_cm = snow / 10) %>%
  mutate(snwd_cm = snwd / 10) %>%
  mutate(tmax = as.numeric(tmax) / 10) %>%
  mutate(tmin = as.numeric(tmin) / 10) %>%
  select(-prcp, -snow, -snwd) 

head(ny_noaa_df)
```

    ## # A tibble: 6 x 9
    ##   id           year month   day  tmax  tmin prcp_cm snow_cm snwd_cm
    ##   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>   <dbl>   <dbl>
    ## 1 US1NYAB0001  2007    11     1    NA    NA      NA      NA      NA
    ## 2 US1NYAB0001  2007    11     2    NA    NA      NA      NA      NA
    ## 3 US1NYAB0001  2007    11     3    NA    NA      NA      NA      NA
    ## 4 US1NYAB0001  2007    11     4    NA    NA      NA      NA      NA
    ## 5 US1NYAB0001  2007    11     5    NA    NA      NA      NA      NA
    ## 6 US1NYAB0001  2007    11     6    NA    NA      NA      NA      NA

``` r
ny_noaa_snowfall <- ny_noaa_df %>%
  group_by(snow_cm) %>%
  count() %>%
  arrange(desc(n))

head(ny_noaa_snowfall)
```

    ## # A tibble: 6 x 2
    ## # Groups:   snow_cm [6]
    ##   snow_cm       n
    ##     <dbl>   <int>
    ## 1     0   2008508
    ## 2    NA    381221
    ## 3     2.5   31022
    ## 4     1.3   23095
    ## 5     5.1   18274
    ## 6     7.6   10173

0.0 is the most commonly observed snowfall value because it does not
snow year round in New York. Most of the time it is not snowing. After
NA (missing), the next most common snowfall amounts are 2.5cm and 1.3cm.

Bullet 2: data manipulation followed by plotting. want january and july
and avg max temp in each station across years group by (station, year,
month) and then summarize, then filter by month (want january and july
only). each station have avg max temp. plot those. avg max temp over
years. many line stacked on each other. use facet for two panel plot.
what structure appears? january hotter now? global warming happening?
some stations always colder?

Make a two-panel plot showing the average max temperature in January and
in July in each station across years. Is there any observable /
interpretable structure? Any outliers?

``` r
noaa_jan_july_df <- ny_noaa_df  %>% 
  filter((month == 1) | (month == 7)) %>%
  group_by(id, year, month) %>%
  summarize(avg_max_temp = mean(tmax, na.rm=TRUE))
```

    ## `summarise()` regrouping output by 'id', 'year' (override with `.groups` argument)

``` r
noaa_jan_july_df %>%
  ggplot(aes(x = year, y = avg_max_temp, color = id)) + 
  geom_line() + 
  facet_grid(. ~ month) +
  labs(
    title = "Average Max Temperature Across Years for January vs July",
    x = "Year",
    y = "Average Max Temperature (C)",
    caption = "Data from the New York NOAA weather stations across years for January and July ") +
    theme(legend.position = "none") +
    theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))
```

    ## Warning: Removed 5640 row(s) containing missing values (geom_path).

![](p8105_hw3_ts3349_files/figure-gfm/problem%203%20bullet%202-1.png)<!-- -->
Both graphs show a zigzag pattern where the average max temperature dips
a little one year but then increases a little the next year. There seems
to be a very slight upward trend for January but overall the average max
temperatures for January seem fairly consistent. The average max
temperature in July seem very consistent overall. Some stations in both
graphs seem to always be slightly higher or slightly lower overall than
the others. In the graph for July there seems to be a station that is an
outlier in the late 80s with a large dip in the average max temperature.
In the year 2000 for January there seems to be a station with a much
higher average max temperature than the others.

Make a two-panel plot showing the average max temperature in January and
in July in each station across years. Is there any observable /
interpretable structure? Any outliers?

Bullet 3: two distinct plots need to be joined via patchwork. 1st
plot-contour or hex plot not scatterplot. 2nd plot-filter and then show
distribution (boxplot, violin, ridge, etc)-one box for each year.

Make a two-panel plot showing (i) tmax vs tmin for the full dataset
(note that a scatterplot may not be the best option); and (ii) make a
plot showing the distribution of snowfall values greater than 0 and less
than 100 separately by year

``` r
tmax_vs_tmin_plot <- ny_noaa_df %>%
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex() + 
  labs(
    title = "Average Max Temperature vs Average Min Temperature",
    x = "Max Temperature (C)",
    y = "Min Temperature (C)",
    caption = "Data from the New York NOAA weather stations") 


snowfall_plot <- ny_noaa_df  %>% 
  filter((snow_cm > 0) & (snow_cm < 100)) %>%
  ggplot(aes(x = as.character(year), y = snow_cm)) + 
  geom_violin() + 
  labs(
    title = "Snowfall Across Years (Greater than 0 and Less than 100 cm)",
    x = "Year",
    y = "Snowfall (cm)",
    caption = "Data from the New York NOAA weather stations")  +
  theme(axis.text.x = element_text(angle = 270, vjust = 0.5, hjust = 1))

tmax_vs_tmin_plot / snowfall_plot
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

![](p8105_hw3_ts3349_files/figure-gfm/problem%203%20bullet%203-1.png)<!-- -->
